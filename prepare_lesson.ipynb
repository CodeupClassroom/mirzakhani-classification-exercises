{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "\n",
    "Plan - Acquire - **Prepare** - Explore - Model - Deliver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we are doing and why:\n",
    "\n",
    "**What:** Clean and tidy our data so that it is ready for exploration, analysis and modeling\n",
    "\n",
    "**Why:** Set ourselves up for certainty! \n",
    "\n",
    "    1) Ensure that our observations will be sound:\n",
    "        Validity of statistical and human observations\n",
    "    2) Ensure that we will not have computational errors:\n",
    "        non numerical data cells, nulls/NaNs\n",
    "    3) Protect against overfitting:\n",
    "        Ensure that have a split data structure prior to drawing conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High level Roadmap:\n",
    "\n",
    "**Input:** An aquired dataset (One Pandas Dataframe) \n",
    "\n",
    "**Output:** Cleaned data split into Train, Validate, and Test sets (Three Pandas Dataframes)\n",
    "\n",
    "**Processes:** Inspect and summarize the data ---> Clean the data ---> Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect and Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# train test split from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# imputer from sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# filter out warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# our own acquire script:\n",
    "import acquire "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Variable |\tDescription\t| Details |\n",
    "|---|---|---|\n",
    "passenger_id| Index| Unique| \n",
    "survival | Survived the crisis |0 = No; 1 = Yes|\n",
    "pclass\t|Passenger Class\t|1 = 1st; 2 = 2nd; 3 = 3rd|\n",
    "sex\t|Sex| \"male\", \"female\" | \n",
    "age|Age\t| |\n",
    "sibsp\t|Number of Siblings/Spouses Aboard|\t |\n",
    "parch\t|Number of Parents/Children Aboard|\t |\n",
    "fare\t|Passenger Fare|\t| \n",
    "embarked\t|Port of Embarkation|\tC = Cherbourg; Q = Queenstown; S = Southampton|\n",
    "deck | Location of cabin| |\n",
    "embarked_town| Port of Embarkation| |\n",
    "alone| Registered as a solo traveler | | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect and Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing our data\n",
    "df = acquire.get_titanic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "- **Continuous Variables**\n",
    "    - `age`, `fare`\n",
    "    \n",
    "- **Categorical Variables**\n",
    "    - `survived`, `pclass`, `sex`, `sibsp`, `parch`, `embarked`, `class`, `deck`, `embark_town`, `alone`\n",
    "\n",
    "**Notes**:\n",
    "- `passenger_id` is effectively an index and provides no predictive quality\n",
    "- `survived` is our target variable\n",
    "- `embarked` and `embark_town` seem to be identical information (not identical data, but identical info...what's the difference?)\n",
    "- `pclass` and `class` also seem to be identical\n",
    "- Redundant columns will need to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at relationship between embarked and embark_town\n",
    "pd.crosstab(df.embarked, df.embark_town)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`embarked` and `embark_town` contain identical information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed look at the relationship between \n",
    "pd.crosstab(df['class'], df.pclass) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`class` and `pclass` contain identical information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['sibsp'], df['alone'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">71 passengers had no siblings or spouses aboard, but were not marked as being alone. Perhaps they are children? We could look at `parch` for this. Something to explore later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `df.info()` will give us a quick view of the datatypes (Dtype) and the nulls in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "- There is a substantial number of nulls in `deck`\n",
    "- There are 2 nulls in `embarked`\n",
    "- There are 100+ nulls in `age`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review summary statistics of numeric columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig deeper into each of the fields\n",
    "- For categorical columns, we can look at `value_counts()`\n",
    "- For numeric columns, we can look as histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of our numeric columns\n",
    "numcols = [col for col in df.columns if df[col].dtype != 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of our categorical columns\n",
    "catcols = [col for col in df.columns if df[col].dtype == 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the object columns\n",
    "for col in catcols:\n",
    "    print(f\"Column: {col}\")\n",
    "    print(df[col].value_counts())\n",
    "    print(\"--------\")\n",
    "    print(df[col].value_counts(normalize=True, dropna=False))\n",
    "    print(\"=================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of numeric columns\n",
    "for col in numcols:\n",
    "    print(col)\n",
    "    df[col].hist()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT NOTE: Visualizations create through a loop should only be part of your personal exploration. Do not include this much noise in a report or presentation!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "1. Removal\n",
    "- Remove `embarked`\n",
    "- Remove `pclass`\n",
    "- Remove `passenger_id`\n",
    "- Remove `deck`\n",
    "    - Has too many nulls\n",
    "    - Would require an extensive imputation process\n",
    "        - Build this out after an MVP is acheived\n",
    "        \n",
    "2. Imputing Nulls\n",
    "- Lots of missing information in `age`\n",
    "    - Going to have to impute nulls\n",
    "- Two nulls in `embark_town`\n",
    "    - Going to have to impute these nulls (maybe just use mode)\n",
    "    \n",
    "3. Encoding categorical variables\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape # No duplicates after all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop redundant columns (and `deck` because it has too many nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['embarked', 'pclass', 'passenger_id', 'deck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = columns_to_drop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding: Turning Categorical Values into Boolean Values (0,1)\n",
    " - We have two options: simple encoding or one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding steps\n",
    "# 1. Make a dataframe out of \"dummy\" columns\n",
    "# 2. Concatenate our dummy dataframe to our original dataframe\n",
    "\n",
    "dummy_df = pd.get_dummies(df[['sex', 'class', 'embark_town']], dummy_na=False, drop_first=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate my dummy_df to my data\n",
    "\n",
    "df = pd.concat([df, dummy_df], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting our Work Into a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_titanic_data(df):\n",
    "    '''\n",
    "    Takes in a titanic dataframe and returns a cleaned dataframe\n",
    "    Arguments: df - a pandas dataframe with the expected feature names and columns\n",
    "    Return: clean_df - a dataframe with the cleaning operations performed on it\n",
    "    '''\n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    # Drop columns \n",
    "    columns_to_drop = ['embarked', 'pclass', 'passenger_id', 'deck']\n",
    "    df = df.drop(columns = columns_to_drop)\n",
    "    # encoded categorical variables\n",
    "    dummy_df = pd.get_dummies(df[['sex', 'class', 'embark_town']], dummy_na=False, drop_first=[True, True])\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_titanic_data()\n",
    "clean_df = clean_titanic_data(df)\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have two columns with nulls:\n",
    "1. `age`\n",
    "2. `embark_town`\n",
    "\n",
    "As a general practice, wait until after the Train, Validate, Test Split before filling nulls.\n",
    "\n",
    "### WHY?\n",
    "\n",
    "> Note: There can be cases where it is okay to fill nulls before splitting. We will talk about those cases after we get through creating the Train, Validate, Test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validate, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(clean_df,\n",
    "                               train_size = 0.8,\n",
    "                               stratify = clean_df.survived,\n",
    "                               random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate = train_test_split(train,\n",
    "                                  train_size = 0.7,\n",
    "                                  stratify = train.survived,\n",
    "                                  random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option for Missing Values: Impute\n",
    "\n",
    "We can impute values using the mean, median, mode (most frequent), or a constant value. We will use sklearn.imputer.SimpleImputer to do this.  \n",
    "\n",
    "1. Create the imputer object, selecting the strategy used to impute (mean, median or mode (strategy = 'most_frequent'). \n",
    "2. Fit to train. This means compute the mean, median, or most_frequent (i.e. mode) for each of the columns that will be imputed. Store that value in the imputer object. \n",
    "3. Transform train: fill missing values in train dataset with that value identified\n",
    "4. Transform test: fill missing values with that value identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create the `SimpleImputer` object, which we will store in the variable `imputer`. In the creation of the object, we will specify the strategy to use (`mean`, `median`, `most_frequent`). Essentially, this is creating the instructions and assigning them to a variable we will reference.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean', missing_values=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(imputer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `Fit` the imputer to the columns in the training df.  This means that the imputer will determine the `most_frequent` value, or other value depending on the `strategy` called, for each column.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = imputer.fit(train[['age']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. It will store that value in the imputer object to use upon calling `transform.` We will call `transform` on each of our samples to fill any missing values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train[['age']] = imputer.transform(train[['age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate[['age']] = imputer.transform(validate[['age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['age']] = imputer.transform(test[['age']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will run through all of these steps, when I provide a train and test dataframe, a strategy, and a list of columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def impute_age(train, validate, test):\n",
    "    '''\n",
    "    Imputes the mean age of train to all three datasets\n",
    "    '''\n",
    "    imputer = SimpleImputer(strategy='mean', missing_values=np.nan)\n",
    "    imputer = imputer.fit(train[['age']])\n",
    "    train[['age']] = imputer.transform(train[['age']])\n",
    "    validate[['age']] = imputer.transform(validate[['age']])\n",
    "    test[['age']] = imputer.transform(test[['age']])\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blend the clean, split and impute functions into a single prep_data() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep_titanic_data(df): \n",
    "    df = clean_titanic_data(df)\n",
    "    train, test = train_test_split(df,\n",
    "                               train_size = 0.8,\n",
    "                               stratify = df.survived,\n",
    "                               random_state=1234)\n",
    "    train, validate = train_test_split(train,\n",
    "                                  train_size = 0.7,\n",
    "                                  stratify = train.survived,\n",
    "                                  random_state=1234)\n",
    "    train, validate, test = impute_age(train, validate, test)\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_titanic_data()\n",
    "train, validate, test = prep_titanic_data(df)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How should we impute `embark_town`?**\n",
    "- `SimpleImputer()`\n",
    "- `.fillna()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "The end product of this exercise should be the specified functions in a python script named `prepare.py`.\n",
    "Do these in your `classification_exercises.ipynb` first, then transfer to the prepare.py file. \n",
    "\n",
    "This work should all be saved in your local `classification-exercises` repo. Then add, commit, and push your changes.\n",
    "\n",
    "**Using the Iris Dataset:**  \n",
    "\n",
    "1. Use the function defined in `acquire.py` to load the iris data.  \n",
    "\n",
    "1. Drop the `species_id` and `measurement_id` columns.  \n",
    "\n",
    "1. Rename the `species_name` column to just `species`.  \n",
    "\n",
    "1. Create dummy variables of the species name. \n",
    "\n",
    "1. Create a function named `prep_iris` that accepts the untransformed iris data, and returns the data with the transformations above applied.  \n",
    "\n",
    "**Using the Titanic Dataset:**\n",
    "\n",
    "1. Use the function defined in acquire.py to load the Titanic data.\n",
    "\n",
    "1. Drop any unnecessary, unhelpful, or duplicated columns.\n",
    "\n",
    "1. Encode the categorical columns. Create dummy variables of the categorical columns and concatenate them onto the dataframe.\n",
    "\n",
    "1. Create a function named `prep_titanic` that accepts the raw titanic data, and returns the data with the transformations above applied.\n",
    "\n",
    "**Using the Telco Dataset:**\n",
    "\n",
    "1. Use the function defined in `acquire.py` to load the Telco data.\n",
    "\n",
    "1. Drop any unnecessary, unhelpful, or duplicated columns. This could mean dropping foreign key columns but keeping the corresponding string values, for example.\n",
    "\n",
    "1. Encode the categorical columns. Create dummy variables of the categorical columns and concatenate them onto the dataframe.\n",
    "\n",
    "1. Create a function named `prep_telco` that accepts the raw telco data, and returns the data with the transformations above applied.\n",
    "\n",
    "**Split your data**\n",
    "\n",
    "1. Write a function to split your data into `train`, `validate`, and `test` datasets. Add this function to `prepare.py`.\n",
    "\n",
    "1. Run the function in your notebook on the Iris dataset, returning 3 datasets: `train_iris`, `validate_iris`, and `test_iris`.\n",
    "\n",
    "1. Run the function on the Titanic dataset, returning 3 datasets: `train_titanic`, `validate_titanic`, and `test_titanic`.\n",
    "\n",
    "1. Run the function on the Telco dataset, returning 3 datasets: `train_telco`, `validate_telco`, and `test_telco`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
