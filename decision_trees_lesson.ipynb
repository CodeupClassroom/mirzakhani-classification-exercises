{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c07dbd",
   "metadata": {},
   "source": [
    "# Decision Tree Lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3cd1c7-800a-4481-b4e1-8dc0bf17e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General DS Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d986166-7524-447a-9ff1-67b393194056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree and Model Evaluation Imports\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01dba08",
   "metadata": {},
   "source": [
    "### Question: Can we predict whether someone is a smoker based on the tips data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70453686",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(25)\n",
    "tips = sns.load_dataset('tips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d83674",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d019f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = pd.get_dummies(tips, columns=['sex', 'day', 'time'], drop_first=True)\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b795c1-4e18-4ac1-9148-f0a84d3c2380",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tips[['total_bill', 'tip', 'size']]\n",
    "y = tips.smoker\n",
    "\n",
    "X_train_and_validate, X_test, y_train_and_validate, y_test = train_test_split(X, y, random_state=123, test_size=.3)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train_and_validate, y_train_and_validate, random_state=123, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddfc28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd6aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690fdd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a877b",
   "metadata": {},
   "source": [
    "## Decision Tree Modeling\n",
    "\n",
    "#### Step 1: Create Your Recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0107552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the decision tree object and specify hyperparams\n",
    "tree = DecisionTreeClassifier(max_depth = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5192e",
   "metadata": {},
   "source": [
    "#### Step 2: Fit your model to your in-sample data (train)\n",
    "\n",
    "Until a model learns from your data, its just an algorithm. After it has learned and changed its algorithmic structure (i.e. has decided which questions to ask and in what order), then it can be called a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad66da9-5de3-484a-890d-ef4edcaa9923",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Rudimentary visualization of model structure\n",
    "print(export_text(tree, feature_names=X_train.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094fb5b9-fdf5-4159-8018-f7bf0fdbb79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tree\n",
    "plt.figure(figsize=(12, 7))\n",
    "plot_tree(tree, feature_names=X_train.columns, class_names=y_train.unique())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c72317",
   "metadata": {},
   "source": [
    "#### Step 3: Use your model to make predictions on the in-sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b075b9d-3a96-4045-893e-364979316f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dfbc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tree.predict(X_train)\n",
    "actual = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db82f7",
   "metadata": {},
   "source": [
    "#### Step 4: Evaluate your predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac00579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(actual, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(actual, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05028fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(tree, X_train, y_train, display_labels=['Non-Smoker', 'Smoker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(actual, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c942d5",
   "metadata": {},
   "source": [
    "#### Step 5: Use best n models on out-of-sample data (validate)\n",
    "\n",
    "In this case, I only have one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae89297-a2b1-46e6-9664-909c92f6faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tree.predict(X_validate)\n",
    "actual = y_validate\n",
    "\n",
    "print(classification_report(actual, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a415f7ea",
   "metadata": {},
   "source": [
    "#### Step 6: Use single best model to make predictions from out-of-sample data (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b2df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tree.predict(X_test)\n",
    "actual = y_test\n",
    "\n",
    "print(classification_report(actual, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1684e3",
   "metadata": {},
   "source": [
    "#### Bonus: What happens if we change some of the hyperparameters?\n",
    "\n",
    "Let's try changing max depth to none. This model will ask questions until every leaf gini is as small as absolutely possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad11fa7-2a45-4a93-87e6-a322d38c1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the decision tree object and specify hyperparams\n",
    "tree = DecisionTreeClassifier(max_depth=None) \n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(export_text(tree, feature_names=X_train.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c78d376-f9af-4bd6-a117-c2621c92ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9), dpi=300)\n",
    "plot_tree(tree, feature_names=X_train.columns, class_names=y_train.unique())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76971a1a",
   "metadata": {},
   "source": [
    "#### How accurate is our crazy model on our training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9123324-49f5-47fa-acd0-fcff54958dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.score(X_train, y_train) # 100%!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede3cd1",
   "metadata": {},
   "source": [
    "#### How accurate is our crazy model on out-of-sample data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d812af-d575-4eb3-b064-e5be4e080d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.score(X_validate, y_validate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c174ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validate.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b105eb0f",
   "metadata": {},
   "source": [
    "We would have had better accuracy just guessing \"No\" on everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33ef35c-6112-48c8-ab0c-cea86954d243",
   "metadata": {},
   "source": [
    "## What did we miss? What can we do next?\n",
    "\n",
    "- Making a baseline model and properly evaluating it\n",
    "- Make more models, try out different combinations of independent variables and hyperparameters\n",
    "> **Note**: Our X can change, but our y remains constant\n",
    "- Quantify and compare those models (on validate!) against each other and the baseline\n",
    "\n",
    "## Pruning\n",
    "Decision Trees have a high risk of being overfit to our training data. We can use `max_depth` and `min_samples` as a way to reduce overfitting. We could build a process that exhaustively checked every hyperparameter combination or we can use an existing process:\n",
    "- Look into [`cost_complexity_pruning_path`](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html)\n",
    "\n",
    "# Exercises\n",
    "\n",
    "Using the titanic data, in your classification-exercises repository, create a notebook, `model.ipynb` where you will do the following:\n",
    "\n",
    "1. What is your baseline prediction? What is your baseline accuracy? remember: your baseline prediction for a classification problem is predicting the most prevelant class in the training dataset (the mode). When you make those predictions, what is your accuracy? This is your baseline accuracy.\n",
    "\n",
    "2. Fit the decision tree classifier to your training sample and transform (i.e. make predictions on the training sample)\n",
    "\n",
    "3. Evaluate your in-sample results using the model score, confusion matrix, and classification report.\n",
    "\n",
    "4. Compute: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support.\n",
    "\n",
    "5. Run through steps 2-4 using a different max_depth value.\n",
    "\n",
    "6. Which model performs better on your in-sample data?\n",
    "\n",
    "7. Which model performs best on your out-of-sample data, the validate set?\n",
    "\n",
    "8. Work through these same exercises using the Telco dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a570e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
